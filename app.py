import streamlit as st
import requests
import pandas as pd
import urllib3
import logging
import io
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Optional, Any

# === è¨­å®šå€ ===
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# è¨­å®š Logging (åœ¨ Streamlit ä¸­ï¼Œé€™æœƒè¼¸å‡ºåˆ°å¾Œå° Terminal)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# è¨­å®šç¶²é æ¨™é¡Œèˆ‡å¯¬åº¦ä½ˆå±€
st.set_page_config(page_title="åœ‹æ³°åŸºé‡‘æ·¨å€¼åˆ†æ", layout="wide")

class Config:
    """å…¨åŸŸé…ç½®é¡åˆ¥"""
    API_URL = "https://www.cathaybk.com.tw/cathaybk/service/newwealth/fund/chartservice.asmx/GetFundNavChart"
    BASE_URL = "https://www.cathaybk.com.tw/cathaybk/personal/investment/fund/details/?fundid={}"
    USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    TIMEOUT = 10
    DEFAULT_DATE_FROM = "1900/01/01"
    
    # é€™è£¡ç›´æ¥å®šç¾©é è¨­æ¸…å–®ï¼Œæ–¹ä¾¿ç¶²é è¼¸å…¥æ¡†ä½¿ç”¨
    DEFAULT_FUND_IDS_LIST = [
        "00580030", "00400013", "00060004", "00100045", "00010144", "00120001",
        "00040097", "10340003", "10350005", "00060003", "00400029", "00100046",
        "00010074", "0074B059", "0012C007", "0012C004", "0012C033", "0012C035",
        "0012C008", "00100118", "00400156", "00400104", "00040052", "10020058",
        "10110022", "0074B065", "00100058", "00580062", "10310016", "00100063",
        "00560011", "00400072"
    ]


class FundScraper:
    """è² è²¬ç¶²è·¯è«‹æ±‚èˆ‡è³‡æ–™æŠ“å– (æ ¸å¿ƒé‚è¼¯ä¸è®Š)"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({"User-Agent": Config.USER_AGENT})
        self.session.verify = False 

    def fetch_nav(self, fund_id: str) -> Optional[pd.DataFrame]:
        target_url = Config.BASE_URL.format(fund_id)
        payload = {"req": {"Keys": [fund_id], "From": Config.DEFAULT_DATE_FROM}}
        headers = {"Referer": target_url}

        try:
            resp = self.session.post(
                Config.API_URL, json=payload, headers=headers, timeout=Config.TIMEOUT
            )
            resp.raise_for_status()
            data_json = resp.json()

            if not data_json.get('Data'):
                return None

            fund_info = data_json['Data'][0]
            df = pd.DataFrame(fund_info['data'], columns=['timestamp', 'NAV'])
            
            df['æ—¥æœŸ'] = pd.to_datetime(df['timestamp'], unit='ms').dt.date
            df['åŸºé‡‘åç¨±'] = fund_info['name']
            df['URL'] = target_url
            
            return df[['æ—¥æœŸ', 'NAV', 'åŸºé‡‘åç¨±', 'URL']]

        except Exception as e:
            logger.error(f"å–å¾—åŸºé‡‘ {fund_id} å¤±æ•—: {e}")
            return None

    def fetch_all_funds(self, fund_ids: List[str], progress_bar=None) -> Dict[str, pd.DataFrame]:
        """
        æ–°å¢ progress_bar åƒæ•¸ï¼Œç”¨ä¾†æ›´æ–°ç¶²é ä¸Šçš„é€²åº¦æ¢
        """
        results = {}
        total = len(fund_ids)
        completed = 0
        
        with ThreadPoolExecutor(max_workers=10) as executor:
            future_to_id = {executor.submit(self.fetch_nav, fid): fid for fid in fund_ids}
            
            for future in as_completed(future_to_id):
                fid = future_to_id[future]
                try:
                    df = future.result()
                    if df is not None:
                        results[fid] = df
                except Exception as e:
                    logger.error(f"Error {fid}: {e}")
                
                # æ›´æ–°é€²åº¦æ¢
                completed += 1
                if progress_bar:
                    progress_bar.progress(completed / total, text=f"æ­£åœ¨æŠ“å–... ({completed}/{total})")
        
        return results


class FundAnalyzer:
    """è² è²¬è¨ˆç®—é‚è¼¯ (æ ¸å¿ƒé‚è¼¯ä¸è®Š)"""
    
    @staticmethod
    def analyze_single(df: pd.DataFrame) -> Dict[str, Any]:
        df = df.sort_values('æ—¥æœŸ')
        fund_name = df['åŸºé‡‘åç¨±'].iloc[0]
        url = df['URL'].iloc[0]

        latest = df.iloc[-1]
        
        hist_max_idx = df['NAV'].idxmax()
        hist_min_idx = df['NAV'].idxmin()

        one_year_ago = df['æ—¥æœŸ'].max() - timedelta(days=365)
        df_1y = df[df['æ—¥æœŸ'] >= one_year_ago]
        
        if df_1y.empty:
            max_1y, min_1y = None, None
            max_1y_date, min_1y_date = None, None
        else:
            max_1y_idx = df_1y['NAV'].idxmax()
            min_1y_idx = df_1y['NAV'].idxmin()
            max_1y = df_1y.loc[max_1y_idx, 'NAV']
            max_1y_date = df_1y.loc[max_1y_idx, 'æ—¥æœŸ']
            min_1y = df_1y.loc[min_1y_idx, 'NAV']
            min_1y_date = df_1y.loc[min_1y_idx, 'æ—¥æœŸ']

        return {
            "åŸºé‡‘åç¨±": fund_name,
            "åŸºé‡‘é€£çµ": url,
            "æœ€æ–°åƒ¹æ ¼": latest['NAV'],
            "æœ€æ–°åƒ¹æ ¼æ—¥æœŸ": latest['æ—¥æœŸ'],
            "æ­·å²æœ€é«˜åƒ¹æ ¼": df.loc[hist_max_idx, 'NAV'],
            "æ­·å²æœ€é«˜åƒ¹æ ¼æ—¥æœŸ": df.loc[hist_max_idx, 'æ—¥æœŸ'],
            "æ­·å²æœ€ä½åƒ¹æ ¼": df.loc[hist_min_idx, 'NAV'],
            "æ­·å²æœ€ä½åƒ¹æ ¼æ—¥æœŸ": df.loc[hist_min_idx, 'æ—¥æœŸ'],
            "è¿‘ä¸€å¹´æœ€é«˜åƒ¹æ ¼": max_1y,
            "è¿‘ä¸€å¹´æœ€é«˜åƒ¹æ ¼æ—¥æœŸ": max_1y_date,
            "è¿‘ä¸€å¹´æœ€ä½åƒ¹æ ¼": min_1y,
            "è¿‘ä¸€å¹´æœ€ä½åƒ¹æ ¼æ—¥æœŸ": min_1y_date
        }

    @staticmethod
    def analyze_all(data_map: Dict[str, pd.DataFrame]) -> pd.DataFrame:
        summary_list = []
        for df in data_map.values():
            summary_list.append(FundAnalyzer.analyze_single(df))
        return pd.DataFrame(summary_list)


class ExcelReport:
    """Excel ç”¢ç”Ÿå™¨ï¼šä¿®æ”¹ç‚ºå¯«å…¥ BytesIO è¨˜æ†¶é«”"""
    
    @staticmethod
    def create_excel_bytes(summary_df: pd.DataFrame) -> bytes:
        """ç”¢ç”Ÿ Excel æª”æ¡ˆä¸¦å›å‚³äºŒé€²ä½è³‡æ–™ (bytes)"""
        output = io.BytesIO()
        
        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            display_df = summary_df.drop(columns=['åŸºé‡‘é€£çµ'])
            display_df.to_excel(writer, index=False, header=False, sheet_name='Summary', startrow=1)

            workbook = writer.book
            worksheet = writer.sheets['Summary']
            
            # ä½¿ç”¨æˆ‘å€‘ä¹‹å‰å®šç¾©çš„æ¨£å¼é‚è¼¯
            ExcelReport._apply_styles(workbook, worksheet, display_df, summary_df)
            ExcelReport._set_columns_width(display_df, worksheet)
            
            worksheet.freeze_panes(1, 0)
        
        return output.getvalue()

    @staticmethod
    def _apply_styles(workbook, worksheet, display_df, original_df):
        base_font = 'Microsoft JhengHei'
        styles = {
            'header': workbook.add_format({'bold': True, 'font_name': base_font, 'bg_color': '#DCE6F1', 'align': 'center', 'valign': 'vcenter', 'border': 1}),
            'text': workbook.add_format({'font_name': base_font, 'text_wrap': True, 'valign': 'top', 'border': 1}),
            'link': workbook.add_format({'font_color': 'blue', 'underline': 1, 'font_name': base_font, 'valign': 'top', 'border': 1}),
            'date': workbook.add_format({'num_format': 'yyyy-mm-dd', 'font_name': base_font, 'valign': 'top', 'border': 1})
        }

        for col_num, value in enumerate(display_df.columns):
            worksheet.write(0, col_num, value, styles['header'])

        date_cols = [idx for idx, col in enumerate(display_df.columns) if 'æ—¥æœŸ' in str(col)]
        rows, cols = display_df.shape

        for i in range(rows):
            fund_name = display_df.iat[i, 0]
            url = original_df.iloc[i]['åŸºé‡‘é€£çµ']
            worksheet.write_url(i + 1, 0, url, styles['link'], string=fund_name)

            for j in range(1, cols):
                val = display_df.iat[i, j]
                if j in date_cols and pd.notna(val):
                    if isinstance(val, (str, datetime, pd.Timestamp)):
                         val = pd.to_datetime(val)
                    worksheet.write_datetime(i + 1, j, val, styles['date'])
                elif isinstance(val, (int, float)):
                    worksheet.write_number(i + 1, j, val, styles['text'])
                else:
                    worksheet.write(i + 1, j, str(val), styles['text'])

    @staticmethod
    def _set_columns_width(df, worksheet):
        for i, col in enumerate(df.columns):
            max_len = max(
                df[col].astype(str).map(lambda x: len(x.encode('utf-8'))).max(),
                len(str(col).encode('utf-8'))
            )
            width = min(max(max_len * 0.9, 10), 50)
            worksheet.set_column(i, i, width)


def main():
    # === ç¶²é ä»‹é¢è¨­è¨ˆ ===
    st.title("ğŸ“Š åœ‹æ³°åŸºé‡‘æ·¨å€¼è‡ªå‹•åˆ†æå·¥å…·")
    st.markdown("æ­¤å·¥å…·å”åŠ©æ‚¨è‡ªå‹•æŠ“å–åœ‹æ³°åŸºé‡‘æ­·å²æ·¨å€¼ï¼Œè¨ˆç®—è¿‘ä¸€å¹´é«˜ä½é»ï¼Œä¸¦ç”Ÿæˆ Excel å ±è¡¨ã€‚")

    # 1. å´é‚Šæ¬„ï¼šè¨­å®šåŸºé‡‘æ¸…å–®
    with st.sidebar:
        st.header("âš™ï¸ åŸºé‡‘è¨­å®š")
        default_ids_str = ",\n".join(Config.DEFAULT_FUND_IDS_LIST)
        user_input = st.text_area(
            "è«‹è¼¸å…¥åŸºé‡‘ä»£è™Ÿ (ä»¥é€—è™Ÿæˆ–æ›è¡Œåˆ†éš”)", 
            value=default_ids_str, 
            height=300,
            help="ä½ å¯ä»¥éš¨æ„æ–°å¢æˆ–åˆªé™¤é€™è£¡çš„ä»£è™Ÿ"
        )
        
        # è™•ç†ä½¿ç”¨è€…è¼¸å…¥
        input_ids = [x.strip() for x in user_input.replace("\n", ",").split(",") if x.strip()]
        st.info(f"ç›®å‰å…±é¸å– {len(input_ids)} æ”¯åŸºé‡‘")

    # 2. ä¸»æŒ‰éˆ•
    if st.button("ğŸš€ é–‹å§‹åˆ†æ", type="primary"):
        if not input_ids:
            st.error("è«‹è‡³å°‘è¼¸å…¥ä¸€æ”¯åŸºé‡‘ä»£è™Ÿï¼")
            return

        # 3. åŸ·è¡ŒæŠ“å–
        scraper = FundScraper()
        
        # å»ºç«‹ä¸€å€‹é€²åº¦æ¢ç‰©ä»¶
        progress_bar = st.progress(0, text="æº–å‚™é–‹å§‹...")
        
        try:
            # å‚³å…¥é€²åº¦æ¢ç‰©ä»¶è®“ Scraper æ›´æ–°
            all_data = scraper.fetch_all_funds(input_ids, progress_bar)
            progress_bar.progress(100, text="ä¸‹è¼‰å®Œæˆï¼é–‹å§‹åˆ†æ...")
        except Exception as e:
            st.error(f"ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            return

        if not all_data:
            st.warning("âš ï¸ æ²’æœ‰æŠ“åˆ°ä»»ä½•è³‡æ–™ï¼Œè«‹æª¢æŸ¥ä»£è™Ÿæ˜¯å¦æ­£ç¢ºã€‚")
            return

        # 4. åŸ·è¡Œåˆ†æ
        summary_df = FundAnalyzer.analyze_all(all_data)
        
        # 5. é¡¯ç¤ºçµæœ
        st.success("âœ… åˆ†æå®Œæˆï¼")
        
        # åœ¨ç¶²é ä¸Šé è¦½å‰ 10 ç­†
        st.subheader("ğŸ“‹ åˆ†æçµæœé è¦½")
        st.dataframe(summary_df.head(10))

        # 6. ä¸‹è¼‰æŒ‰éˆ•
        # å‘¼å«æˆ‘å€‘ä¿®æ”¹éçš„ ExcelReportï¼Œæ‹¿åˆ°äºŒé€²ä½è³‡æ–™
        excel_data = ExcelReport.create_excel_bytes(summary_df)
        
        file_name = f"fund_summary_{datetime.now().strftime('%Y%m%d')}.xlsx"
        
        st.download_button(
            label="ğŸ“¥ ä¸‹è¼‰ Excel å®Œæ•´å ±è¡¨",
            data=excel_data,
            file_name=file_name,
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )

if __name__ == "__main__":
    main()