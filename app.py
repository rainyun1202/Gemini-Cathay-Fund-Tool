import streamlit as st
import requests
import pandas as pd
import urllib3
import logging
import io
import yfinance as yf
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Optional, Any

# === è¨­å®šå€ ===
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

st.set_page_config(page_title="å…¨çƒå¸‚å ´èˆ‡åŸºé‡‘åˆ†æ", layout="wide")

class Config:
    """å…¨åŸŸé…ç½®é¡åˆ¥"""
    # --- åœ‹æ³°åŸºé‡‘è¨­å®š ---
    API_URL = "https://www.cathaybk.com.tw/cathaybk/service/newwealth/fund/chartservice.asmx/GetFundNavChart"
    BASE_URL = "https://www.cathaybk.com.tw/cathaybk/personal/investment/fund/details/?fundid={}"
    USER_AGENT = "Mozilla/5.0"
    TIMEOUT = 10
    DEFAULT_DATE_FROM = "1900/01/01"
    
    DEFAULT_FUND_IDS_LIST = [
        "00580030", "00400013", "00060004", "00100045", "00010144", "00120001",
        "00040097", "10340003", "10350005", "00060003", "00400029", "00100046",
        "00010074", "0074B059", "0012C007", "0012C004", "0012C033", "0012C035",
        "0012C008", "00100118", "00400156", "00400104", "00040052", "10020058",
        "10110022", "0074B065", "00100058", "00580062", "10310016", "00100063",
        "00560011", "00400072"
    ]

    # --- Yahoo Finance å¸‚å ´æŒ‡æ•¸è¨­å®š ---
    MARKET_TICKERS = {
        "æ¯”ç‰¹å¹£ (BTC-USD)": "BTC-USD",
        "VIX ææ…ŒæŒ‡æ•¸": "^VIX",
        "ç¾åœ‹ 10 å¹´æœŸå…¬å‚µæ®–åˆ©ç‡": "^TNX",
        "ç¾å…ƒæŒ‡æ•¸ (DXY)": "DX-Y.NYB",
        "å¸ƒè˜­ç‰¹åŸæ²¹": "BZ=F",
        "é»ƒé‡‘æœŸè²¨": "GC=F",
        "ç¾…ç´  2000": "^RUT",
        "NASDAQ æŒ‡æ•¸": "^IXIC",
        "S&P 500": "^GSPC",
        "è²»åŸåŠå°é«”": "^SOX",
        "ä¸Šè­‰æŒ‡æ•¸": "000001.SS",
        "é¦™æ¸¯åœ‹ä¼æŒ‡æ•¸": "^HSCE"
    }


class FundScraper:
    """è² è²¬æŠ“å–åœ‹æ³°åŸºé‡‘"""
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({"User-Agent": Config.USER_AGENT})
        self.session.verify = False 

    def fetch_nav(self, fund_id: str) -> Optional[pd.DataFrame]:
        target_url = Config.BASE_URL.format(fund_id)
        payload = {"req": {"Keys": [fund_id], "From": Config.DEFAULT_DATE_FROM}}
        headers = {"Referer": target_url}

        try:
            resp = self.session.post(Config.API_URL, json=payload, headers=headers, timeout=Config.TIMEOUT)
            resp.raise_for_status()
            data_json = resp.json()
            if not data_json.get('Data'): return None
            
            fund_info = data_json['Data'][0]
            df = pd.DataFrame(fund_info['data'], columns=['timestamp', 'NAV'])
            df['æ—¥æœŸ'] = pd.to_datetime(df['timestamp'], unit='ms').dt.date
            df['åŸºé‡‘åç¨±'] = fund_info['name']
            df['URL'] = target_url
            return df[['æ—¥æœŸ', 'NAV', 'åŸºé‡‘åç¨±', 'URL']]
        except Exception as e:
            logger.error(f"åŸºé‡‘ {fund_id} å¤±æ•—: {e}")
            return None

    # ç§»é™¤åŸæœ¬çš„ fetch_allï¼Œæ”¹ç‚ºé€éå¤–éƒ¨å‡½å¼å‘¼å«ä»¥æ”¯æ´å¿«å–
    def fetch_single_safe(self, fid):
        """å–®ç´”ç‚ºäº† ThreadPool è¨­è¨ˆçš„è¼”åŠ©å‡½å¼"""
        return self.fetch_nav(fid)


class MarketScraper:
    """è² è²¬æŠ“å– Yahoo Finance å¸‚å ´æ•¸æ“š"""
    def fetch_history(self, name: str, ticker: str) -> Optional[pd.DataFrame]:
        try:
            stock = yf.Ticker(ticker)
            # ä½¿ç”¨ "max" æŠ“å–å®Œæ•´æ­·å²æ•¸æ“š
            hist = stock.history(period="max")
            
            if hist.empty: return None
            
            hist = hist.reset_index()
            hist['Date'] = hist['Date'].dt.date
            
            df = pd.DataFrame()
            df['æ—¥æœŸ'] = hist['Date']
            df['NAV'] = hist['Close']
            df['åŸºé‡‘åç¨±'] = name
            df['URL'] = f"https://finance.yahoo.com/quote/{ticker}"
            
            return df[['æ—¥æœŸ', 'NAV', 'åŸºé‡‘åç¨±', 'URL']]
        except Exception as e:
            logger.error(f"å¸‚å ´æŒ‡æ•¸ {name} å¤±æ•—: {e}")
            return None

# === ã€ç¬¬ä¸€éšæ®µå„ªåŒ–ã€‘å¿«å–å‡½å¼ ===
# é€™äº›å‡½å¼ç¨ç«‹æ–¼ Class ä¹‹å¤–ï¼Œå› ç‚º Streamlit çš„å¿«å–è£é£¾å™¨å° Class method æ”¯æ´åº¦è¼ƒè¤‡é›œ
# ttl=3600 ä»£è¡¨å¿«å–å­˜æ´» 1 å°æ™‚ï¼Œé¿å…è³‡æ–™éèˆŠ
@st.cache_data(ttl=3600, show_spinner=False)
def get_cached_fund_data(fund_ids: List[str]) -> Dict[str, pd.DataFrame]:
    """[å¿«å–] æŠ“å–åœ‹æ³°åŸºé‡‘è³‡æ–™"""
    scraper = FundScraper()
    results = {}
    # ç‚ºäº†é¡¯ç¤ºé€²åº¦ï¼Œæˆ‘å€‘é€™è£¡ç°¡å–®æ¨¡æ“¬ï¼Œå¯¦éš›ä¸Šå› ç‚ºæœ‰å¿«å–ï¼Œç¬¬äºŒæ¬¡åŸ·è¡Œæœƒç¬é–“å®Œæˆ
    with ThreadPoolExecutor(max_workers=10) as executor:
        future_to_id = {executor.submit(scraper.fetch_single_safe, fid): fid for fid in fund_ids}
        for future in as_completed(future_to_id):
            fid = future_to_id[future]
            try:
                df = future.result()
                if df is not None: results[fid] = df
            except Exception: pass
    return results

@st.cache_data(ttl=3600, show_spinner=False)
def get_cached_market_data(market_dict: Dict[str, str]) -> Dict[str, pd.DataFrame]:
    """[å¿«å–] æŠ“å–å¸‚å ´è³‡æ–™"""
    scraper = MarketScraper()
    results = {}
    for name, ticker in market_dict.items():
        df = scraper.fetch_history(name, ticker)
        if df is not None: results[name] = df
    return results


class FundAnalyzer:
    """è² è²¬è¨ˆç®—é‚è¼¯"""
    @staticmethod
    def analyze_single(df: pd.DataFrame) -> Dict[str, Any]:
        df = df.sort_values('æ—¥æœŸ')
        fund_name = df['åŸºé‡‘åç¨±'].iloc[0]
        url = df['URL'].iloc[0]
        latest = df.iloc[-1]
        
        hist_max_idx = df['NAV'].idxmax()
        hist_min_idx = df['NAV'].idxmin()

        one_year_ago = df['æ—¥æœŸ'].max() - timedelta(days=365)
        df_1y = df[df['æ—¥æœŸ'] >= one_year_ago]
        
        if df_1y.empty:
            max_1y, min_1y, max_1y_date, min_1y_date = None, None, None, None
        else:
            max_1y_idx = df_1y['NAV'].idxmax()
            min_1y_idx = df_1y['NAV'].idxmin()
            max_1y = df_1y.loc[max_1y_idx, 'NAV']
            max_1y_date = df_1y.loc[max_1y_idx, 'æ—¥æœŸ']
            min_1y = df_1y.loc[min_1y_idx, 'NAV']
            min_1y_date = df_1y.loc[min_1y_idx, 'æ—¥æœŸ']

        return {
            "åŸºé‡‘åç¨±": fund_name,
            "åŸºé‡‘é€£çµ": url,
            "æœ€æ–°åƒ¹æ ¼": latest['NAV'],
            "æœ€æ–°åƒ¹æ ¼æ—¥æœŸ": latest['æ—¥æœŸ'],
            "æ­·å²æœ€é«˜åƒ¹æ ¼": df.loc[hist_max_idx, 'NAV'],
            "æ­·å²æœ€é«˜åƒ¹æ ¼æ—¥æœŸ": df.loc[hist_max_idx, 'æ—¥æœŸ'],
            "æ­·å²æœ€ä½åƒ¹æ ¼": df.loc[hist_min_idx, 'NAV'],
            "æ­·å²æœ€ä½åƒ¹æ ¼æ—¥æœŸ": df.loc[hist_min_idx, 'æ—¥æœŸ'],
            "è¿‘ä¸€å¹´æœ€é«˜åƒ¹æ ¼": max_1y,
            "è¿‘ä¸€å¹´æœ€é«˜åƒ¹æ ¼æ—¥æœŸ": max_1y_date,
            "è¿‘ä¸€å¹´æœ€ä½åƒ¹æ ¼": min_1y,
            "è¿‘ä¸€å¹´æœ€ä½åƒ¹æ ¼æ—¥æœŸ": min_1y_date
        }

    @staticmethod
    def analyze_all(data_map: Dict[str, pd.DataFrame]) -> pd.DataFrame:
        summary_list = []
        for df in data_map.values():
            summary_list.append(FundAnalyzer.analyze_single(df))
        return pd.DataFrame(summary_list)


class ExcelReport:
    """Excel ç”¢ç”Ÿå™¨"""
    @staticmethod
    def create_excel_bytes(summary_df: pd.DataFrame) -> bytes:
        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            display_df = summary_df.drop(columns=['åŸºé‡‘é€£çµ'])
            display_df.to_excel(writer, index=False, header=False, sheet_name='Summary', startrow=1)

            workbook = writer.book
            worksheet = writer.sheets['Summary']
            
            ExcelReport._apply_styles(workbook, worksheet, display_df, summary_df)
            ExcelReport._set_columns_width(display_df, worksheet)
            
            worksheet.freeze_panes(1, 0)
        return output.getvalue()

    @staticmethod
    def _apply_styles(workbook, worksheet, display_df, original_df):
        base_font = 'Microsoft JhengHei'
        header_fmt = workbook.add_format({'bold': True, 'font_name': base_font, 'bg_color': '#DCE6F1', 'align': 'center', 'valign': 'vcenter', 'border': 1})
        text_fmt = workbook.add_format({'font_name': base_font, 'valign': 'top', 'border': 1})
        num_fmt = workbook.add_format({'font_name': base_font, 'valign': 'top', 'border': 1, 'num_format': '#,##0.00'}) 
        link_fmt = workbook.add_format({'font_color': 'blue', 'underline': 1, 'font_name': base_font, 'valign': 'top', 'border': 1})
        date_fmt = workbook.add_format({'num_format': 'yyyy-mm-dd', 'font_name': base_font, 'valign': 'top', 'border': 1})

        for col, val in enumerate(display_df.columns):
            worksheet.write(0, col, val, header_fmt)

        date_cols = [i for i, c in enumerate(display_df.columns) if 'æ—¥æœŸ' in str(c) or 'Date' in str(c)]
        
        for i in range(len(display_df)):
            name = display_df.iat[i, 0]
            url = original_df.iloc[i]['åŸºé‡‘é€£çµ']
            worksheet.write_url(i+1, 0, url, link_fmt, string=name)

            for j in range(1, len(display_df.columns)):
                val = display_df.iat[i, j]
                if j in date_cols and pd.notna(val):
                    if isinstance(val, (str, datetime, pd.Timestamp)): val = pd.to_datetime(val)
                    worksheet.write_datetime(i+1, j, val, date_fmt)
                elif isinstance(val, (int, float)):
                    worksheet.write_number(i+1, j, val, num_fmt)
                else:
                    worksheet.write(i+1, j, str(val), text_fmt)

    @staticmethod
    def _set_columns_width(df, worksheet):
        for i, col in enumerate(df.columns):
            max_len = max(
                df[col].astype(str).map(lambda x: len(x.encode('utf-8'))).max(),
                len(str(col).encode('utf-8'))
            )
            width = min(max(max_len * 0.9, 10), 50)
            worksheet.set_column(i, i, width)

# === ã€ç¬¬äºŒéšæ®µå„ªåŒ–ã€‘è¦–è¦ºåŒ–è™•ç†é¡åˆ¥ ===
class Visualizer:
    @staticmethod
    def prepare_chart_data(all_data: Dict[str, pd.DataFrame], normalize=False) -> pd.DataFrame:
        """
        å°‡å¤šå€‹ DataFrame åˆä½µç‚ºé©åˆç¹ªåœ–çš„ Wide Format
        normalize: æ˜¯å¦å°‡èµ·é»æ­¸ä¸€åŒ–ç‚º 100% (æ–¹ä¾¿æ¯”è¼ƒæ¼²è·Œå¹…)
        """
        # 1. æå–æ‰€æœ‰è³‡æ–™çš„ 'æ—¥æœŸ' å’Œ 'NAV'
        series_list = []
        for name, df in all_data.items():
            # ç¢ºä¿æ—¥æœŸæ˜¯ datetime æ ¼å¼
            temp_df = df[['æ—¥æœŸ', 'NAV']].copy()
            temp_df['æ—¥æœŸ'] = pd.to_datetime(temp_df['æ—¥æœŸ'])
            temp_df = temp_df.set_index('æ—¥æœŸ')
            temp_df.columns = [name]
            series_list.append(temp_df)
        
        if not series_list:
            return pd.DataFrame()

        # 2. åˆä½µ (Outer Join ä»¥ä¿ç•™æ‰€æœ‰æ—¥æœŸ)
        chart_df = pd.concat(series_list, axis=1).sort_index()
        
        # 3. å¡«è£œç©ºå€¼ (Forward Fill: å‡æ—¥æ²¿ç”¨é€±äº”åƒ¹æ ¼)
        chart_df = chart_df.fillna(method='ffill')
        
        # 4. æ­¸ä¸€åŒ–è™•ç† (å¯é¸)
        if normalize:
            # æ‰¾åˆ°æ¯ä¸€æ¬„ç¬¬ä¸€å€‹éç©ºå€¼ï¼Œå°‡å…¶è¨­ç‚ºåŸºæº–é» (100)
            # é€™æ¨£å¯ä»¥æ¯”è¼ƒä¸åŒåƒ¹æ ¼å€é–“çš„å•†å“ (å¦‚æ¯”ç‰¹å¹£ 90000 vs åŸºé‡‘ 10)
            return chart_df.apply(lambda x: x / x.first_valid_index() * 100 if x.first_valid_index() else x)
        
        return chart_df

def main():
    st.title("ğŸ“Š å…¨çƒå¸‚å ´èˆ‡åŸºé‡‘æ·¨å€¼æˆ°æƒ…å®¤")
    st.markdown("æ•´åˆ **åœ‹æ³°åŸºé‡‘** èˆ‡ **å…¨çƒé—œéµå¸‚å ´æŒ‡æ¨™** çš„è‡ªå‹•åŒ–åˆ†æå·¥å…·ã€‚")

    # === å´é‚Šæ¬„ä½ˆå±€ ===
    with st.sidebar:
        st.header("âš™ï¸ è¨­å®šé¢æ¿")
        
        with st.expander("ğŸŒ å…¨çƒå¸‚å ´æŒ‡æ¨™", expanded=True):
            selected_markets = st.multiselect(
                "é¸æ“‡é—œæ³¨å¸‚å ´æŒ‡æ¨™",
                options=list(Config.MARKET_TICKERS.keys()),
                default=list(Config.MARKET_TICKERS.keys())
            )
            target_markets = {name: Config.MARKET_TICKERS[name] for name in selected_markets}

        with st.expander("ğŸ¦ åœ‹æ³°åŸºé‡‘æ¸…å–®", expanded=True):
            default_ids = ",\n".join(Config.DEFAULT_FUND_IDS_LIST)
            fund_input = st.text_area(
                "åŸºé‡‘ä»£è™Ÿ (æ¯è¡Œä¸€å€‹)", 
                value=default_ids, 
                height=300,
                help="è«‹è¼¸å…¥åŸºé‡‘ä»£è™Ÿï¼Œå¤šç­†è«‹æ›è¡Œæˆ–ç”¨é€—è™Ÿåˆ†éš”"
            )
            fund_ids = [x.strip() for x in fund_input.replace("\n", ",").split(",") if x.strip()]

    # ä¸»ç•«é¢æŒ‰éˆ•
    if st.button("ğŸš€ é–‹å§‹åˆ†æ", type="primary"):
        # é€™è£¡çš„ Spinner æœƒåœ¨è³‡æ–™æŠ“å–æ™‚è½‰åœˆåœˆ
        with st.spinner("æ­£åœ¨é€£ç·šè‡³å…¨çƒè³‡æ–™åº« (è‹¥ç‚ºç¬¬ä¸€æ¬¡æŠ“å–è«‹ç¨å€™)..."):
            all_data = {}
            
            # ä½¿ç”¨å¿«å–å‡½å¼ (ç¬¬ä¸€æ¬¡æ…¢ï¼Œç¬¬äºŒæ¬¡å¿«)
            if target_markets:
                market_data = get_cached_market_data(target_markets)
                all_data.update(market_data)
                
            if fund_ids:
                fund_data = get_cached_fund_data(fund_ids)
                all_data.update(fund_data)
        
        if not all_data:
            st.error("âŒ æœªå–å¾—ä»»ä½•è³‡æ–™ï¼Œè«‹æª¢æŸ¥ç¶²è·¯æˆ–ä»£è™Ÿã€‚")
            return

        # å»ºç«‹é ç±¤ (Tabs) ä¾†å€åˆ†ã€Œæ•¸æ“šå ±è¡¨ã€èˆ‡ã€Œè¶¨å‹¢åœ–è¡¨ã€
        tab1, tab2 = st.tabs(["ğŸ“‹ æ•¸æ“šå ±è¡¨", "ğŸ“ˆ è¶¨å‹¢åœ–è¡¨"])

        with tab1:
            st.success(f"âœ… åˆ†æå®Œæˆï¼å…± {len(all_data)} ç­†æ¨™çš„")
            summary_df = FundAnalyzer.analyze_all(all_data)
            st.dataframe(summary_df.head(10))
            
            excel_data = ExcelReport.create_excel_bytes(summary_df)
            file_name = f"Global_Market_Report_{datetime.now().strftime('%Y%m%d')}.xlsx"
            st.download_button(
                label="ğŸ“¥ ä¸‹è¼‰å®Œæ•´ Excel å ±è¡¨",
                data=excel_data,
                file_name=file_name,
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )

        with tab2:
            st.subheader("æ­·å²èµ°å‹¢æ¯”è¼ƒ")
            
            # æ§åˆ¶é …ï¼šé¸æ“‡è¦ç•«çš„æ¨™çš„
            all_options = list(all_data.keys())
            selected_chart_items = st.multiselect(
                "é¸æ“‡è¦ç¹ªè£½çš„é …ç›® (å»ºè­° 3-5 é …)", 
                options=all_options,
                default=all_options[:3] if len(all_options) >= 3 else all_options
            )
            
            # æ§åˆ¶é …ï¼šæ™‚é–“ç¯„åœ
            col_date1, col_date2 = st.columns(2)
            with col_date1:
                # æ­¸ä¸€åŒ–é–‹é—œ
                normalize_chart = st.checkbox("ğŸ“ˆ æ­¸ä¸€åŒ–æ¯”è¼ƒ (ä»¥èµ·é»ç‚º 100%)", value=True, help="å°‡ä¸åŒåƒ¹æ ¼å–®ä½çš„å•†å“æ”¾åœ¨åŒä¸€å€‹èµ·è·‘é»æ¯”è¼ƒæ¼²è·Œå¹…")
            
            if selected_chart_items:
                # ç¯©é¸å‡ºè¦ç•«åœ–çš„ data
                chart_subset = {k: v for k, v in all_data.items() if k in selected_chart_items}
                
                # æº–å‚™ç¹ªåœ–è³‡æ–™
                chart_df = Visualizer.prepare_chart_data(chart_subset, normalize=normalize_chart)
                
                # ç¹ªè£½äº’å‹•å¼ç·šåœ–
                st.line_chart(chart_df)
            else:
                st.info("è«‹å¾ä¸Šæ–¹é¸å–®é¸æ“‡è‡³å°‘ä¸€å€‹é …ç›®ä¾†ç¹ªè£½åœ–è¡¨")

if __name__ == "__main__":
    main()